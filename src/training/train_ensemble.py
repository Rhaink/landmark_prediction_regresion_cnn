#!/usr/bin/env python3
"""
Entrenamiento Ensemble: M√∫ltiples modelos para Bootstrap Aggregating
Entrena varios modelos ResNet-18 con diferentes random seeds para ensemble learning
"""

import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Any

# Agregar src al path para imports
sys.path.append(str(Path(__file__).parent.parent))

from training.train_phase1 import Phase1Trainer
from training.train_phase2 import Phase2Trainer
from training.utils import load_config, Timer


class EnsembleTrainer:
    """
    Entrenador de Ensemble para landmark prediction

    Estrategia: Bootstrap Aggregating (Bagging)
    - Entrenar m√∫ltiples modelos ResNet-18 id√©nticos
    - Usar diferentes random seeds para diversidad
    - Mismo pipeline de 2 fases para cada modelo
    - Guardado organizado por seed para posterior ensemble
    """

    def __init__(self, config: dict):
        """
        Args:
            config: Configuraci√≥n del entrenamiento ensemble
        """
        self.config = config
        self.ensemble_config = config.get('ensemble', {})

        # Configuraci√≥n del ensemble
        self.num_models = self.ensemble_config.get('num_models', 5)
        self.random_seeds = self.ensemble_config.get('random_seeds', [42, 123, 456, 789, 999])

        # Verificar que tenemos suficientes seeds
        if len(self.random_seeds) < self.num_models:
            # Generar seeds adicionales si es necesario
            import random
            random.seed(42)
            additional_seeds = [random.randint(1, 10000) for _ in range(self.num_models - len(self.random_seeds))]
            self.random_seeds.extend(additional_seeds)

        # Usar solo los primeros num_models seeds
        self.random_seeds = self.random_seeds[:self.num_models]

        # Directorios
        self.checkpoint_dir = Path(config['logging']['checkpoint_dir'])
        self.ensemble_dir = self.checkpoint_dir / "ensemble"
        self.ensemble_dir.mkdir(parents=True, exist_ok=True)

        print("="*70)
        print("ENTRENAMIENTO ENSEMBLE: BOOTSTRAP AGGREGATING")
        print("="*70)
        print(f"üéØ N√∫mero de modelos: {self.num_models}")
        print(f"üé≤ Random seeds: {self.random_seeds}")
        print(f"üìÅ Directorio ensemble: {self.ensemble_dir}")

    def train_phase1_for_seed(self, seed: int) -> str:
        """
        Entrenar Fase 1 para una seed espec√≠fica

        Args:
            seed: Random seed para este modelo

        Returns:
            Ruta al checkpoint de Fase 1 generado
        """
        print(f"\nüîÑ Entrenando Fase 1 para seed {seed}")

        # Crear configuraci√≥n espec√≠fica para este modelo
        model_config = self.config.copy()
        model_config['split']['random_seed'] = seed

        # Crear trainer para Fase 1
        trainer = Phase1Trainer(model_config)

        try:
            # Entrenar Fase 1
            trainer.train()

            # Mover checkpoint con nombre espec√≠fico por seed
            original_checkpoint = trainer.checkpoint_dir / "phase1_best.pt"
            seed_checkpoint = self.checkpoint_dir / f"phase1_seed_{seed}_best.pt"

            if original_checkpoint.exists():
                # Copiar checkpoint con metadatos de seed
                import torch
                checkpoint = torch.load(original_checkpoint)
                checkpoint['ensemble_info'] = {
                    'random_seed': seed,
                    'phase': 1,
                    'ensemble_size': self.num_models
                }
                torch.save(checkpoint, seed_checkpoint)
                print(f"‚úì Fase 1 completada para seed {seed}: {seed_checkpoint}")
                return str(seed_checkpoint)
            else:
                raise FileNotFoundError(f"Checkpoint de Fase 1 no encontrado: {original_checkpoint}")

        except Exception as e:
            print(f"‚ùå Error entrenando Fase 1 para seed {seed}: {e}")
            raise

    def train_single_model(self, seed: int, model_idx: int) -> tuple:
        """
        Entrenar un solo modelo del ensemble (Fase 1 + Fase 2)

        Args:
            seed: Random seed para este modelo
            model_idx: √çndice del modelo (para logging)

        Returns:
            Tupla de (best_val_loss, checkpoint_path)
        """
        print(f"\n" + "="*50)
        print(f"üîÑ MODELO {model_idx+1}/{self.num_models} - SEED {seed}")
        print("="*50)

        # Paso 1: Entrenar Fase 1 si no existe
        phase1_checkpoint = self.checkpoint_dir / f"phase1_seed_{seed}_best.pt"

        if not phase1_checkpoint.exists():
            print(f"üîß Entrenando Fase 1 para seed {seed}...")
            phase1_checkpoint_path = self.train_phase1_for_seed(seed)
        else:
            print(f"‚úì Fase 1 ya existe para seed {seed}: {phase1_checkpoint}")
            phase1_checkpoint_path = str(phase1_checkpoint)

        # Paso 2: Entrenar Fase 2
        print(f"üöÄ Entrenando Fase 2 para seed {seed}...")

        # Crear configuraci√≥n espec√≠fica para este modelo
        model_config = self.config.copy()
        model_config['split']['random_seed'] = seed

        # Crear trainer para Fase 2
        trainer = Phase2Trainer(model_config, phase1_checkpoint_path)

        try:
            # Entrenar modelo
            timer = Timer()
            timer.start()

            best_val_loss = trainer.train()

            timer.stop()

            # Mover checkpoint a directorio ensemble con nombre espec√≠fico
            original_checkpoint = trainer.checkpoint_dir / "phase2_best.pt"
            ensemble_checkpoint = self.ensemble_dir / f"model_{seed}.pt"

            if original_checkpoint.exists():
                # Copiar checkpoint con metadatos adicionales
                import torch
                checkpoint = torch.load(original_checkpoint)
                checkpoint['ensemble_info'] = {
                    'model_index': model_idx,
                    'random_seed': seed,
                    'training_time': timer.elapsed,
                    'ensemble_size': self.num_models
                }
                torch.save(checkpoint, ensemble_checkpoint)
                print(f"‚úì Checkpoint guardado: {ensemble_checkpoint}")
            else:
                raise FileNotFoundError(f"Checkpoint de Fase 2 no encontrado: {original_checkpoint}")

            print(f"‚úÖ Modelo {model_idx+1} completado en {timer.formatted_elapsed()}")
            print(f"üìä Mejor p√©rdida de validaci√≥n: {best_val_loss:.6f}")

            return best_val_loss, str(ensemble_checkpoint)

        except Exception as e:
            print(f"‚ùå Error entrenando modelo {model_idx+1} (seed {seed}): {e}")
            raise

    def train_ensemble(self) -> Dict[str, Any]:
        """
        Entrenar ensemble completo

        Returns:
            Diccionario con estad√≠sticas del ensemble
        """
        print(f"\nüöÄ Iniciando entrenamiento ensemble...")

        # Ahora entrenamos autom√°ticamente Fase 1 + Fase 2 para cada seed

        # Variables de tracking
        ensemble_results = []
        total_timer = Timer()
        total_timer.start()

        print(f"\nüìà Entrenando {self.num_models} modelos...")

        # Entrenar cada modelo del ensemble
        for model_idx, seed in enumerate(self.random_seeds):
            try:
                best_loss, checkpoint_path = self.train_single_model(seed, model_idx)

                ensemble_results.append({
                    'model_index': model_idx,
                    'seed': seed,
                    'best_val_loss': best_loss,
                    'checkpoint_path': checkpoint_path,
                    'status': 'completed'
                })

            except Exception as e:
                print(f"‚ö† Saltando modelo {model_idx+1} debido a error: {e}")
                ensemble_results.append({
                    'model_index': model_idx,
                    'seed': seed,
                    'best_val_loss': None,
                    'checkpoint_path': None,
                    'status': 'failed',
                    'error': str(e)
                })

        total_timer.stop()

        # Calcular estad√≠sticas
        completed_models = [r for r in ensemble_results if r['status'] == 'completed']
        failed_models = [r for r in ensemble_results if r['status'] == 'failed']

        if completed_models:
            losses = [r['best_val_loss'] for r in completed_models]
            avg_loss = sum(losses) / len(losses)
            min_loss = min(losses)
            max_loss = max(losses)
            std_loss = (sum((l - avg_loss) ** 2 for l in losses) / len(losses)) ** 0.5
        else:
            avg_loss = min_loss = max_loss = std_loss = None

        # Guardar metadata del ensemble
        ensemble_metadata = {
            'num_models_requested': self.num_models,
            'num_models_completed': len(completed_models),
            'num_models_failed': len(failed_models),
            'random_seeds': self.random_seeds,
            'training_time_total': total_timer.elapsed,
            'statistics': {
                'avg_val_loss': avg_loss,
                'min_val_loss': min_loss,
                'max_val_loss': max_loss,
                'std_val_loss': std_loss
            },
            'model_results': ensemble_results,
            'config': self.ensemble_config
        }

        # Guardar metadata
        import json
        metadata_path = self.ensemble_dir / "ensemble_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(ensemble_metadata, f, indent=2, default=str)

        # Mostrar resumen
        print(f"\n" + "="*70)
        print("üéâ ENTRENAMIENTO ENSEMBLE COMPLETADO")
        print("="*70)
        print(f"‚è± Tiempo total: {total_timer.formatted_elapsed()}")
        print(f"‚úÖ Modelos completados: {len(completed_models)}/{self.num_models}")

        if failed_models:
            print(f"‚ùå Modelos fallidos: {len(failed_models)}")

        if completed_models:
            print(f"üìä Estad√≠sticas de p√©rdida de validaci√≥n:")
            print(f"   ‚Ä¢ Promedio: {avg_loss:.6f}")
            print(f"   ‚Ä¢ M√≠nimo: {min_loss:.6f}")
            print(f"   ‚Ä¢ M√°ximo: {max_loss:.6f}")
            print(f"   ‚Ä¢ Desviaci√≥n est√°ndar: {std_loss:.6f}")

        print(f"üìÅ Checkpoints guardados en: {self.ensemble_dir}")
        print(f"üìã Metadata guardada en: {metadata_path}")

        if len(completed_models) >= 3:  # M√≠nimo viable para ensemble
            print("‚ú® Ensemble listo para evaluaci√≥n!")
            ensemble_metadata['status'] = 'ready'
        else:
            print("‚ö† Ensemble incompleto - se recomienda al menos 3 modelos")
            ensemble_metadata['status'] = 'incomplete'

        return ensemble_metadata


def main():
    """
    Funci√≥n principal de entrenamiento ensemble
    """
    # Cargar configuraci√≥n
    config_path = "configs/config.yaml"
    if not os.path.exists(config_path):
        print(f"‚ùå Archivo de configuraci√≥n no encontrado: {config_path}")
        return

    config = load_config(config_path)

    # Verificar configuraci√≥n de ensemble
    if 'ensemble' not in config:
        print("‚ùå Configuraci√≥n de ensemble no encontrada en config.yaml")
        print("üí° Agrega una secci√≥n 'ensemble' a la configuraci√≥n")
        return

    # Crear entrenador ensemble
    trainer = EnsembleTrainer(config)

    try:
        results = trainer.train_ensemble()

        if results.get('status') == 'ready':
            print(f"\nüéØ Ensemble entrenado exitosamente!")
            print(f"üîó Para evaluar: python main.py evaluate_ensemble")
        elif results.get('status') == 'incomplete':
            print(f"\n‚ö† Ensemble parcialmente completado")
        else:
            print(f"\n‚ùå Entrenamiento ensemble fall√≥")

    except KeyboardInterrupt:
        print("\n‚ö† Entrenamiento interrumpido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error durante entrenamiento ensemble: {e}")
        raise


if __name__ == "__main__":
    main()